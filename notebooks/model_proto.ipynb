{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a rough prototype of our RL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd() == '/kaggle/working':\n",
    "    import sys\n",
    "    \n",
    "    !rm -rf 'PMLDL-Project'\n",
    "    !git clone -b dev https://github.com/Sambura/PMLDL-Project\n",
    "    \n",
    "    if 'PMLDL-Project' not in sys.path: sys.path.append('PMLDL-Project')\n",
    "\n",
    "    %pip install dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from src.utils.common import *\n",
    "from src.utils.plot_utils import *\n",
    "from src.dnd.actions import *\n",
    "from src.dnd.units import *\n",
    "from src.dnd.FieldGenerator import FieldGenerator\n",
    "from src.dnd.game_utils import *\n",
    "from src.agent.agent import DnDAgent\n",
    "from src.agent.deep_q_network import *\n",
    "from src.agent.agent_utils import get_legal_action_resolver, get_states, agents_play_loop\n",
    "from src.agent.training import *\n",
    "from src.dnd.game_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1234, deterministic_cudnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample agent game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size, game_config = get_2v2_1_config((8, 8))\n",
    "agent = DnDAgent(board_shape=board_size, in_channels=DnDBoard.STATE_CHANNEL_COUNT, out_actions=2, \n",
    "    lr=1e-3, \n",
    "    epsilon=0.9, \n",
    "    min_epsilon=0.01,\n",
    "    epsilon_delta=4e-5, \n",
    "    epsilon_strategy='exp',\n",
    "    gamma=0.9,\n",
    "    batch_size=32, \n",
    "    dual_learning=True,\n",
    "    replace_model_interval=5000,\n",
    "    memory_capacity=100000,\n",
    "    random_action_resolver=get_legal_action_resolver(board_size, False),\n",
    "    model_class=DnDEvalModel,\n",
    "    sequential_actions=False\n",
    ")\n",
    "\n",
    "train_loop = train_loop_trivial\n",
    "reward_policy = calculate_reward_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = 50000\n",
    "checkpoint_interval = 4000\n",
    "game_iter_limit = 2500\n",
    "average_result_over = 1000\n",
    "\n",
    "# set to None to disable report / figures export\n",
    "output_path = '../rnd/'\n",
    "generation_name = '_temp'\n",
    "gen_header = \\\n",
    "'''# ??\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, eps, checkpoints, reps = [], [], [], []\n",
    "mag1s, mag2s = [], []\n",
    "\n",
    "did_replace = [ False ]\n",
    "def on_replace():\n",
    "    global did_replace\n",
    "    did_replace[0] = True\n",
    "    \n",
    "agent.on_replace = on_replace\n",
    "gen = FieldGenerator(board_size=board_size).load_from_folder('../Tokens')\n",
    "\n",
    "report_results = output_path is not None\n",
    "if report_results:\n",
    "    gen_path = os.path.join(output_path, generation_name)\n",
    "    os.makedirs(gen_path, exist_ok=generation_name.startswith('_temp'))\n",
    "else: gen_path = '..' # so that checkpoints still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update = time.time()\n",
    "\n",
    "last_checkpoint = checkpoints[-1] if len(checkpoints) > 0 else 0\n",
    "with tqdm(range(games), desc='Training', smoothing=0.1) as pbar:\n",
    "    for i in pbar:\n",
    "        # gen.reset()\n",
    "        # game = gen.generate_balanced_game(targetCR=1)\n",
    "        game = generate_balanced_game(board_size, game_config)\n",
    "        \n",
    "        outputs = agent.predict(game.observe_board())\n",
    "        metric = np.mean(np.abs(outputs), axis=(1, 2))\n",
    "        mag1s.append(metric[0])\n",
    "        mag2s.append(metric[1])\n",
    "        pbar.set_postfix({'mag1': metric[0], 'mag2': metric[1]})\n",
    "\n",
    "        iter_count = train_loop(agent, game, reward_policy, iter_limit=game_iter_limit, raise_on_limit=False)\n",
    "        iters.append(iter_count)\n",
    "        eps.append(agent.epsilon)\n",
    "\n",
    "        ctime = time.time()\n",
    "        if ctime - last_update > 20:\n",
    "            clear_output(wait=True)\n",
    "            plot_training_history(iters, eps, checkpoints, vlines=reps, min_ymax=30, average_last=average_result_over)\n",
    "            if hasattr(pbar, 'container'): display(pbar.container)\n",
    "            last_update = ctime\n",
    "\n",
    "        iter_i = len(iters)\n",
    "        if iter_i - last_checkpoint >= checkpoint_interval:\n",
    "            last_checkpoint = iter_i\n",
    "            checkpoints.append(iter_i)\n",
    "            avg = np.mean(iters[-average_result_over:]) # recent performance\n",
    "            agent.save_agent(os.path.join(gen_path, f'checkpoints/agent-{avg:0.1f}i-{iter_i / 1000:0.1f}k'))\n",
    "\n",
    "        if did_replace[0]:\n",
    "            did_replace[0] = False\n",
    "            reps.append(iter_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(iters, eps, checkpoints, vlines=reps, min_ymax=30, figsize=(20, 10), show=False)\n",
    "if report_results:\n",
    "    plt.savefig(os.path.join(gen_path, 'training.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mag1s, label='coords')\n",
    "plt.plot(mag2s, label='action')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "if report_results:\n",
    "    plt.savefig(os.path.join(gen_path, 'magnitude.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Total number of iterations: {np.sum(iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_results:\n",
    "    actual_games = len(iters)\n",
    "\n",
    "    with open(os.path.join(gen_path, 'report.md'), 'w') as report:\n",
    "        avg = np.mean(iters[-1000:])\n",
    "        total = np.sum(iters)\n",
    "        perf = 1e9 / (avg * total)\n",
    "        report.write(\n",
    "f'''{gen_header}\n",
    "# Results\n",
    "![image](training.png \"\")\n",
    "\n",
    "* Average iteration count after {actual_games / 1000:0.1f}k games: {avg:0.2f}\n",
    "* Total iterations performed: {total}\n",
    "* Relative training performance: {perf:0.2f}\n",
    "> 1e9 / (total_iter * avg_iter)\n",
    "\n",
    "## Model's outputs magnitude history:\n",
    "\n",
    "![magnitudes](magnitude.png \"\")\n",
    "\n",
    "# Configuration\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "games = {actual_games}\n",
    "```\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shortest game: {np.min(iters)}')\n",
    "print(f'Longest game: {np.max(iters)}')\n",
    "print(f'Number of shortest games: {np.sum(np.array(iters) == np.min(iters))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "game, colormap = decorate_game(gen.generate_balanced_game(targetCR=1))\n",
    "\n",
    "agents_play_loop(agent, agent, game, colormap, reset_epsilon=False, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "game, colormap = decorate_game(gen.generate_balanced_game(targetCR=1))\n",
    "print_game(game, colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.observe_board()\n",
    "plot_featuremaps(\n",
    "    state, \n",
    "    title='Observed state', \n",
    "    fm_names=game.CHANNEL_NAMES,\n",
    "    vmin = -3,\n",
    "    vmax = 3,\n",
    "    separate_cbars=True,\n",
    "    cmap='Greens'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.observe_board()\n",
    "output = agent.predict(state)\n",
    "\n",
    "plot_featuremaps(\n",
    "    output, \n",
    "    title='Agent output', \n",
    "    fm_names=['New coords', 'Target unit'],\n",
    "    separate_cbars=True\n",
    ")\n",
    "\n",
    "print(agent.choose_action_vector(state))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
