{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a rough prototype of our RL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from src.utils.common import *\n",
    "from src.utils.plot_utils import *\n",
    "from src.dnd.actions import *\n",
    "from src.dnd.units import *\n",
    "from src.dnd.game_utils import *\n",
    "from src.dnd.game_board import DnDBoard, GameState\n",
    "from src.agent.agent import DnDAgent, IdleDnDAgent, RandomAgent\n",
    "from src.agent.deep_q_network import *\n",
    "from src.agent.agent_utils import get_legal_action_resolver, get_states, self_play_loop\n",
    "from src.dnd.game_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1234, deterministic_cudnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample agent game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ava_reward_trivial(game, data_agent, data_enemy):\n",
    "    game_state, unit, player_id, move_legal, action_legal, updates = data_agent\n",
    "\n",
    "    units_removed = updates['units_removed']\n",
    "    reward = 0\n",
    "    # reward for removing enemy units, 1 for each unit\n",
    "    reward += len([x for x in units_removed if x[1] != player_id]) * 1\n",
    "    # reward for winning\n",
    "    if len(game.players_to_units[player_id]) == len(game.units):\n",
    "        reward += 10\n",
    "    #if data_enemy is not None and data_enemy[0] == GameState.WIN:\n",
    "    #    reward -= 10\n",
    "    # penalty for losing (on your own turn ??)\n",
    "    if len(game.players_to_units[player_id]) == 0:\n",
    "        reward = -100\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ava_simple(agent: DnDAgent,\n",
    "                          enemy: DnDAgent,\n",
    "                          game: DnDBoard, \n",
    "                          reward_fn: callable,\n",
    "                          iter_limit: int=10000) -> tuple[int, bool]:\n",
    "    last_state, last_action, last_turn_info = None, None, None\n",
    "    \n",
    "    for iter_count in range(iter_limit):\n",
    "        if game.current_player_id == 0: # agent turn\n",
    "            last_state, last_action, new_coords, action = get_states(game, agent)\n",
    "            last_turn_info = game.take_turn(new_coords, action, skip_illegal=True)\n",
    "            game_over = last_turn_info[0] != GameState.PLAYING\n",
    "\n",
    "            if game.current_player_id == 0: # if the next move is agent's move again, memorize current transition\n",
    "                reward = reward_fn(game, last_turn_info, None)\n",
    "                new_state = game.observe_board()\n",
    "                agent.memorize(last_state, last_action, reward, new_state, game_over)\n",
    "                agent.learn()\n",
    "            \n",
    "            if game_over:\n",
    "                return iter_count + 1, last_turn_info[0] == GameState.WIN\n",
    "\n",
    "        else: # enemy turn\n",
    "            _, _, new_coords, action = get_states(game, enemy)\n",
    "            enemy_turn_info = game.take_turn(new_coords, action, skip_illegal=True)\n",
    "            game_over = enemy_turn_info[0] != GameState.PLAYING\n",
    "\n",
    "            if game.current_player_id == 0 and last_state is not None: # if the next move is agent's, memorize the transition\n",
    "                new_state = game.observe_board()\n",
    "                reward = reward_fn(game, last_turn_info, enemy_turn_info)\n",
    "                agent.memorize(last_state, last_action, reward, new_state, game_over)\n",
    "                agent.learn()\n",
    "\n",
    "            if game_over:\n",
    "                return iter_count + 1, enemy_turn_info[0] == GameState.LOSE\n",
    "\n",
    "    raise RuntimeError('Iteration limit exceeded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = (6, 6)\n",
    "game_config = get_2v2_1_config(board_size)\n",
    "\n",
    "agent = DnDAgent(board_shape=board_size, in_channels=8, out_actions=2, \n",
    "    lr=1e-3,\n",
    "    epsilon=0.2, \n",
    "    min_epsilon=0.01,\n",
    "    epsilon_delta=1e-5, \n",
    "    epsilon_strategy='linear',\n",
    "    gamma=0.9,\n",
    "    batch_size=32,\n",
    "    dual_learning=True, \n",
    "    replace_model_interval=5000,\n",
    "    memory_capacity=100000,\n",
    "    random_action_resolver=get_legal_action_resolver(board_size),\n",
    "    model_class=DnDEvalModelRT\n",
    ")\n",
    "\n",
    "replace_interval_delta = 50\n",
    "train_loop = train_loop_ava_simple\n",
    "reward_policy = ava_reward_trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_agent_paths = [\n",
    "    # '../rnd/2v2-0/trained-agents/agent-gen27-12.7i',\n",
    "    #'../rnd/2v2-0/trained-agents/agent-gen26-10.3i',\n",
    "    #'../rnd/2v2-0/trained-agents/agent-gen15-10.2i',\n",
    "]\n",
    "\n",
    "ref_agents = [(DnDAgent.load_agent(x, strip=True, epsilon=0), x.split('/')[-1]) for x in ref_agent_paths]\n",
    "ref_agents.append((IdleDnDAgent(), 'Idle agent'))\n",
    "ref_agents.append((RandomAgent(board_size, 2, action_resolver=get_legal_action_resolver(board_size)), 'Random agent'))\n",
    "ref_agents.append((agent, 'Self play'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_path = '../rnd/2v2-1/'\n",
    "generation_name = 'gen1'\n",
    "gen_header = \\\n",
    "'''# AVA based agent. Trained against one of the best 2v2-0 agents, random agent, and idle agent.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, eps, checkpoints, reps, wins = [], [], [], [], []\n",
    "mag1s, mag2s = [], []\n",
    "\n",
    "did_replace = [ False ]\n",
    "def on_replace():\n",
    "    global did_replace\n",
    "    did_replace[0] = True\n",
    "    \n",
    "agent.on_replace = on_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ava_training_history(wins, ref_agents, \n",
    "                              eps=None, \n",
    "                              checkpoints=None,\n",
    "                              vlines=None,\n",
    "                              xlim=None, \n",
    "                              #ylim=None, \n",
    "                              figsize=(11, 6), \n",
    "                              show=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title('Winrate')\n",
    "    plt.xlabel('Game')\n",
    "    plt.ylabel('Winrate')\n",
    "    if xlim is None: xlim = [None, None]\n",
    "    else: xlim = list(xlim)\n",
    "    if xlim[0] is None: xlim[0] = 0\n",
    "    if xlim[1] is None: xlim[1] = len(wins)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(-0.4, 100.4)\n",
    "\n",
    "    ax: plt.Axes = plt.gca()\n",
    "\n",
    "    agent_cnt = len(ref_agents)\n",
    "\n",
    "    per_agent_wins = [(list(range(x, len(wins), agent_cnt)), wins[x::agent_cnt]) for x in range(agent_cnt)]\n",
    "    \n",
    "    for (xs, ys), (_, name) in zip(per_agent_wins, ref_agents):\n",
    "        if len(ys) == 0: continue\n",
    "        n = 30\n",
    "        win_conv = 100 * np.convolve(ys, np.ones(n) / n, mode='valid')\n",
    "        ax.plot(np.linspace(n // 2, len(wins) - n // 2, len(win_conv)), win_conv, lw=1)\n",
    "\n",
    "        n = 300\n",
    "        win_conv = 100 * np.convolve(ys, np.ones(n) / n, mode='valid')\n",
    "        ax.plot(np.linspace(n // 2, len(wins) - n // 2, len(win_conv)), win_conv, lw=3, label=name)\n",
    "\n",
    "    ax.axhline(50, color='k', lw=2, linestyle='--')\n",
    "    ax.grid()\n",
    "\n",
    "    artists, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    ax2 = None\n",
    "    if eps is not None:\n",
    "        ax2 = plt.twinx()\n",
    "        ax2.plot(eps, color='red', label='Epsilon')\n",
    "\n",
    "        for x, y in zip((artists, labels), ax2.get_legend_handles_labels()): x += y\n",
    "\n",
    "    if vlines is None: vlines = []\n",
    "    vlines = vlines.copy()\n",
    "    if len(vlines) > 0 and not hasattr(vlines[0], '__len__'): vlines = [vlines]\n",
    "    if checkpoints is not None: vlines.append({'data': checkpoints, 'c': 'k', 'alpha': 0.6, 'lw': 1.5, 'linestyle': '--'})\n",
    "\n",
    "    for vline in vlines:\n",
    "        if isinstance(vline, dict):\n",
    "            points = vline.pop('data')\n",
    "            kwargs = vline\n",
    "        else:\n",
    "            points = vline\n",
    "            kwargs = { 'c': 'red', 'lw': 1, 'linestyle': '--', 'alpha': 0.8}\n",
    "            \n",
    "        for point in points:\n",
    "            ax.axvline(point, **kwargs)\n",
    "\n",
    "    ax.legend(artists, labels, loc='upper right')\n",
    "\n",
    "    if show: plt.show()\n",
    "    else: return (ax, ax2) if ax2 is not None else ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = 50000\n",
    "checkpoint_each = 5000\n",
    "checkpoint_name = f'2v2-0-agent-{generation_name}'\n",
    "if checkpoint_name is None: checkpoint_each = 1e99\n",
    "last_update = -1e9\n",
    "\n",
    "last_checkpoint = checkpoints[-1] if len(checkpoints) > 0 else 0\n",
    "with tqdm(range(games), desc='Training', smoothing=0.1) as pbar:\n",
    "    for i in pbar:\n",
    "        iter_i = len(iters)\n",
    "        ref_agent = ref_agents[iter_i % len(ref_agents)][0]\n",
    "        game = generate_balanced_game(*game_config)\n",
    "        outputs = agent.predict(game.observe_board())\n",
    "        metric = np.mean(np.abs(outputs), axis=(1, 2))\n",
    "        iter_count, win = train_loop(agent, ref_agent, game, reward_policy, iter_limit=100000)\n",
    "        iters.append(iter_count)\n",
    "        wins.append(win)\n",
    "        eps.append(agent.epsilon)\n",
    "        mag1s.append(metric[0])\n",
    "        mag2s.append(metric[1])\n",
    "        pbar.set_postfix({'mag1': metric[0], 'mag2': metric[1]})\n",
    "\n",
    "        ctime = time.time()\n",
    "        if ctime - last_update > 30:\n",
    "            clear_output(wait=True)\n",
    "            plot_ava_training_history(wins, ref_agents, eps, checkpoints, vlines=reps)\n",
    "            display(pbar.container)\n",
    "            last_update = ctime\n",
    "\n",
    "        if iter_i - last_checkpoint >= checkpoint_each:\n",
    "            last_checkpoint = iter_i\n",
    "            checkpoints.append(iter_i)\n",
    "            agent.save_agent(f'../checkpoints/{checkpoint_name}-{iter_i}')\n",
    "\n",
    "        if did_replace[0]:\n",
    "            did_replace[0] = False\n",
    "            reps.append(iter_i)\n",
    "            agent.replace_model_interval += replace_interval_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = os.path.join(reports_path, generation_name)\n",
    "os.makedirs(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ava_training_history(wins, ref_agents, eps, checkpoints, vlines=reps, show=False)\n",
    "plt.savefig(os.path.join(export_path, 'winrate.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(iters, eps, checkpoints, vlines=reps, ylim=50, figsize=(20, 10), show=False)\n",
    "plt.savefig(os.path.join(export_path, 'training.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(mag1s[:650000], label='coords')\n",
    "plt.plot(mag2s[:650000], label='action')\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "#plt.savefig(os.path.join(export_path, 'magnitude.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Total number of iterations: {np.sum(iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(export_path, 'report.md'), 'w') as report:\n",
    "    avg = np.mean(iters[-1000:])\n",
    "    total = np.sum(iters)\n",
    "    perf = 1e9 / (avg * total)\n",
    "    report.write(\n",
    "f'''{gen_header}\n",
    "# Results\n",
    "![image](training.png \"\")\n",
    "\n",
    "* Average iteration count after 20k games: {avg:0.2f}\n",
    "* Total iterations performed: {total}\n",
    "* Relative training performance: {perf:0.2f}\n",
    "> 1e9 / (total_iter * avg_iter)\n",
    "\n",
    "## Model's outputs magnitude history:\n",
    "\n",
    "![magnitudes](magnitude.png \"\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shortest game: {np.min(iters)}')\n",
    "print(f'Longest game: {np.max(iters)}')\n",
    "print(f'Number of shortest games: {np.sum(np.array(iters) == np.min(iters))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, color_map = create_same_game()\n",
    "\n",
    "self_play_loop(agent, game, color_map, reset_epsilon=True, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.observe_board()\n",
    "plot_featuremaps(\n",
    "    state, \n",
    "    title='Observed state', \n",
    "    fm_names=game.CHANNEL_NAMES,\n",
    "    vmin = -3,\n",
    "    vmax = 3,\n",
    "    separate_cbars=True,\n",
    "    cmap='Greens'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, color_map = create_same_game()\n",
    "agent.game = game\n",
    "print_game(game, color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.observe_board()\n",
    "output = agent.predict(state)\n",
    "\n",
    "plot_featuremaps(\n",
    "    output, \n",
    "    title='Agent output', \n",
    "    fm_names=['New coords', 'Target unit'],\n",
    "    separate_cbars=True\n",
    ")\n",
    "\n",
    "print(agent.choose_action_vector(state))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
