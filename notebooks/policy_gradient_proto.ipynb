{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a rough prototype of our RL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd() == '/kaggle/working':\n",
    "    import sys\n",
    "    \n",
    "    !rm -rf 'PMLDL-Project'\n",
    "    !git clone -b main https://github.com/Sambura/PMLDL-Project\n",
    "    \n",
    "    if 'PMLDL-Project' not in sys.path: sys.path.append('PMLDL-Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "from src.utils.common import *\n",
    "from src.utils.plot_utils import *\n",
    "from src.dnd.actions import *\n",
    "from src.dnd.units import *\n",
    "from src.dnd.game_utils import *\n",
    "from src.agent.agent_pg import DnDAgentPolicyGradient\n",
    "from src.agent.deep_q_network import *\n",
    "from src.agent.agent_utils import get_legal_action_resolver, get_states, agents_play_loop\n",
    "from src.agent.training import *\n",
    "from src.dnd.game_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1234, deterministic_cudnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample agent game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_sequential_V1_PG(agent: DnDAgent, \n",
    "                                game: DnDBoard,\n",
    "                                reward_fn: callable,\n",
    "                                iter_limit: int=10000,\n",
    "                                raise_on_limit: bool=False) -> int:\n",
    "    if not agent.sequential_actions:\n",
    "        raise RuntimeWarning('Provided agent is incompatible with this train loop')\n",
    "\n",
    "    # enemy_memory = []\n",
    "    for iter_count in range(iter_limit):\n",
    "        unit, player_id = game.current_unit, game.current_player_id\n",
    "\n",
    "        while game.current_movement_left > 0 or not game.used_action: # while unit is still able to do something\n",
    "            state, action_vector, new_coords, action = get_states_seq(game, agent)\n",
    "            action_legal, move_legal = None, None\n",
    "\n",
    "            if new_coords is not None: # move to new_coords\n",
    "                move_legal, updates = game.move(new_coords, raise_on_illegal=False)\n",
    "                finish_turn = not move_legal\n",
    "            elif action is not None: # invoke the action\n",
    "                action_legal, updates = game.use_action(action, raise_on_illegal=False)\n",
    "                finish_turn = not action_legal\n",
    "            else:\n",
    "                finish_turn = True\n",
    "                updates = None\n",
    "\n",
    "            game_state = game.get_game_state(player_id)\n",
    "            reward = reward_fn(game, game_state, unit, player_id, move_legal, action_legal, updates)\n",
    "            if player_id == 0: \n",
    "                agent.memorize(state, action_vector, reward)\n",
    "            # else:\n",
    "            #     enemy_memory.append((state, action_vector, reward))\n",
    "\n",
    "            if game_state != GameState.PLAYING: \n",
    "                # agent.learn()\n",
    "\n",
    "                # for memory in enemy_memory:\n",
    "                #     agent.memorize(*memory)\n",
    "                # agent.learn()\n",
    "                \n",
    "                return iter_count + 1\n",
    "\n",
    "            if finish_turn: break\n",
    "\n",
    "        game.finish_turn()\n",
    "    \n",
    "    if raise_on_limit: raise RuntimeError('Iteration limit exceeded')\n",
    "\n",
    "    # agent.learn()\n",
    "# \n",
    "    # for memory in enemy_memory:\n",
    "    #     agent.memorize(*memory)\n",
    "    # agent.learn()\n",
    "\n",
    "    return iter_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_classic_seq_PG(game, game_state, unit: Unit, player_id: int, move_legal: bool, action_legal: bool, updates: dict):\n",
    "    if move_legal is None and action_legal is None: return -0.1\n",
    "\n",
    "    if len(game.players_to_units[player_id]) == 0:\n",
    "        return -1\n",
    "    \n",
    "    if len(game.players_to_units[player_id]) == len(game.units):\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size, game_config = get_2v2_1_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi, xi = np.meshgrid(np.arange(board_size[0]), np.arange(board_size[1]), indexing='ij')\n",
    "\n",
    "def dnd_legal_moves_filter(state, probs):\n",
    "    remaining_speed = state[11, 0, 0]\n",
    "\n",
    "    current_unit_pos = np.where(state[2] != 0)\n",
    "    y, x = current_unit_pos[0][0], current_unit_pos[1][0]        \n",
    "    distance = np.abs(yi - y) + np.abs(xi - x)\n",
    "\n",
    "    attack_range = state[4, y, x]\n",
    "    possible_targets = np.where(np.logical_and(state[1], distance <= attack_range))\n",
    "\n",
    "    occupied = np.logical_or(state[0], state[1])\n",
    "    possible_positions = np.where(np.logical_and(distance <= remaining_speed, occupied == 0))\n",
    "\n",
    "    filtered_map = np.zeros_like(probs)\n",
    "\n",
    "    filtered_map[0, possible_positions[0], possible_positions[1]] = probs[0, possible_positions[0], possible_positions[1]]\n",
    "    filtered_map[1, possible_targets[0], possible_targets[1]] = probs[1, possible_targets[0], possible_targets[1]]\n",
    "    filtered_map[2, 0, 0] = probs[2, 0, 0]\n",
    "\n",
    "    return filtered_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#board_size = (8, 8)\n",
    "\n",
    "agent = DnDAgentPolicyGradient(board_shape=board_size, in_channels=DnDBoard.STATE_CHANNEL_COUNT, out_actions=3, \n",
    "    lr=1e-4, \n",
    "    gamma=0.99,\n",
    "    batch_size=320000, \n",
    "    memory_capacity=50000,\n",
    "    model_class=DnDEvalModelRT19,\n",
    "    sequential_actions=True,\n",
    "    legal_moves_filter=dnd_legal_moves_filter\n",
    ")\n",
    "\n",
    "train_loop = train_loop_sequential_V1_PG\n",
    "reward_policy = calculate_reward_classic_seq_PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = 50000\n",
    "checkpoint_interval = 1000\n",
    "game_iter_limit = 2500\n",
    "average_result_over = 1000\n",
    "\n",
    "# set to None to disable report / figures export\n",
    "output_path = '../rnd/'\n",
    "generation_name = '_temp-pg'\n",
    "gen_header = \\\n",
    "'''# New field generation test\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, checkpoints, rewards = [], [], []\n",
    "mag1s, mag2s = [], []\n",
    "\n",
    "def pg_memorize(state, actions, reward, new_state, game_over):\n",
    "    agent.memorize(state, actions, reward)\n",
    "\n",
    "gen = fieldGenerator(board_size=board_size).load_from_folder('../Tokens')\n",
    "\n",
    "report_results = output_path is not None\n",
    "if report_results:\n",
    "    gen_path = os.path.join(output_path, generation_name)\n",
    "    os.makedirs(gen_path, exist_ok=generation_name.startswith('_temp'))\n",
    "else: gen_path = '..' # so that checkpoints still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update = time.time()\n",
    "\n",
    "last_checkpoint = checkpoints[-1] if len(checkpoints) > 0 else 0\n",
    "with tqdm(range(games), desc='Training', smoothing=0.1) as pbar:\n",
    "    for i in pbar:\n",
    "        gen.reset()\n",
    "        game = gen.generate_balanced_game(targetCR=1)\n",
    "        \n",
    "        # outputs = agent.predict(game.observe_board())\n",
    "        # metric = np.mean(np.abs(outputs), axis=(1, 2))\n",
    "        # mag1s.append(metric[0])\n",
    "        # mag2s.append(metric[1])\n",
    "        # pbar.set_postfix({'mag1': metric[0], 'mag2': metric[1]})\n",
    "\n",
    "        iter_count = train_loop(agent, game, reward_policy, iter_limit=game_iter_limit, raise_on_limit=False)\n",
    "        iters.append(iter_count)\n",
    "        rewards.append(np.mean(agent.reward_memory[:agent.memory_position]))\n",
    "        agent.learn()\n",
    "\n",
    "        if i == 10000:\n",
    "            agent.set_lr(1e-5)\n",
    "\n",
    "        ctime = time.time()\n",
    "        if ctime - last_update > 20:\n",
    "            clear_output(wait=True)\n",
    "            plot_training_history(iters, checkpoints=checkpoints, min_ymax=20, average_last=average_result_over)\n",
    "            #plt.plot(rewards)\n",
    "            plt.plot(np.convolve(rewards, np.ones(50) / 50, mode='same')[:-25])\n",
    "            plt.axhline(0, c='k')\n",
    "            plt.show()\n",
    "            if hasattr(pbar, 'container'): display(pbar.container)\n",
    "            last_update = ctime\n",
    "\n",
    "        iter_i = len(iters)\n",
    "        if iter_i - last_checkpoint >= checkpoint_interval:\n",
    "            last_checkpoint = iter_i\n",
    "            checkpoints.append(iter_i)\n",
    "            avg = np.mean(iters[-average_result_over:]) # recent performance\n",
    "            agent.save_agent(os.path.join(gen_path, f'checkpoints/agent-{avg:0.1f}i-{iter_i / 1000:0.1f}k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(iters, eps, checkpoints, vlines=reps, min_ymax=20, figsize=(20, 10), show=False)\n",
    "if report_results:\n",
    "    plt.savefig(os.path.join(gen_path, 'training.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mag1s, label='coords')\n",
    "plt.plot(mag2s, label='action')\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "if report_results:\n",
    "    plt.savefig(os.path.join(gen_path, 'magnitude.png'), bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Total number of iterations: {np.sum(iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_results:\n",
    "    actual_games = len(iters)\n",
    "\n",
    "    with open(os.path.join(gen_path, 'report.md'), 'w') as report:\n",
    "        avg = np.mean(iters[-1000:])\n",
    "        total = np.sum(iters)\n",
    "        perf = 1e9 / (avg * total)\n",
    "        report.write(\n",
    "f'''{gen_header}\n",
    "# Results\n",
    "![image](training.png \"\")\n",
    "\n",
    "* Average iteration count after {actual_games / 1000:0.1f}k games: {avg:0.2f}\n",
    "* Total iterations performed: {total}\n",
    "* Relative training performance: {perf:0.2f}\n",
    "> 1e9 / (total_iter * avg_iter)\n",
    "\n",
    "## Model's outputs magnitude history:\n",
    "\n",
    "![magnitudes](magnitude.png \"\")\n",
    "\n",
    "# Configuration\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "games = {actual_games}\n",
    "```\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shortest game: {np.min(iters)}')\n",
    "print(f'Longest game: {np.max(iters)}')\n",
    "print(f'Number of shortest games: {np.sum(np.array(iters) == np.min(iters))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "game, colormap = decorate_game(generate_balanced_game(board_size, game_config))\n",
    "\n",
    "agents_play_loop([agent, agent], game, colormap, reset_epsilon=False, delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.reset()\n",
    "game, colormap = decorate_game(generate_balanced_game(board_size, game_config))\n",
    "print_game(game, colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.observe_board()\n",
    "plot_featuremaps(\n",
    "    state, \n",
    "    title='Observed state', \n",
    "    fm_names=game.CHANNEL_NAMES,\n",
    "    vmin = -3,\n",
    "    vmax = 3,\n",
    "    separate_cbars=True,\n",
    "    cmap='Greens'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.observe_board()\n",
    "output = agent.predict_probabilities(state)\n",
    "\n",
    "plot_featuremaps(\n",
    "    output, \n",
    "    title='Agent output', \n",
    "    fm_names=['New coords', 'Target unit', 'pass'],\n",
    "    separate_cbars=True\n",
    ")\n",
    "\n",
    "print(agent.choose_single_action(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "game, colormap = decorate_game(generate_balanced_game(board_size, game_config))\n",
    "print_game(game, colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent.agent_utils import agent_take_turn\n",
    "agent_take_turn(game, agent, None, True)\n",
    "#print_game(game, colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_featuremaps(agent.predict(game.observe_board()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
